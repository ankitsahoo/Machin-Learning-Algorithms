### Deep Learning 

Deep learning is a subfield of machine learning that uses neural networks with many layers.

------

### 1. **Artificial Neural Networks (ANN)**

- **What it does**: The basic structure of neural networks with input, hidden, and output layers.
- **Example**: Predicting house prices based on multiple features like size, location, and number of rooms.

------

### 2. **Convolutional Neural Networks (CNN)**

- **What it does**: Processes image and spatial data by identifying patterns like edges and textures.
- **Example**: Detecting faces in photos or classifying objects like cats vs. dogs.

------

### 3. **Recurrent Neural Networks (RNN)**

- **What it does**: Processes sequential data by retaining memory of previous inputs.
- **Example**: Predicting stock prices based on historical trends.

------

### 4. **Long Short-Term Memory (LSTM)**

- **What it does**: A type of RNN that remembers long-term dependencies effectively.
- **Example**: Predicting the next word in a sentence or language translation tasks.

------

### 5. **Gated Recurrent Units (GRU)**

- **What it does**: A simpler and faster alternative to LSTM, capturing sequence dependencies.
- **Example**: Predicting weather patterns based on time-series data.

------

### 6. **Generative Adversarial Networks (GANs)**

- **What it does**: Comprises two networks (generator and discriminator) that compete to create realistic data.
- **Example**: Generating realistic images like deepfake videos or creating artwork.

------

### 7. **Transformer Networks**

- **What it does**: Uses self-attention mechanisms for processing large sequences, essential for NLP.
- **Example**: Powering language models like ChatGPT or Google Translate.

------

### 8. **Variational Autoencoders (VAE)**

- **What it does**: A type of autoencoder used for generating new data while keeping it close to the original distribution.
- **Example**: Creating synthetic handwritten digits or designing new drug molecules.

------

### 9. **Deep Belief Networks (DBN)**

- **What it does**: Probabilistic generative models that learn hierarchical features.
- **Example**: Recognizing handwritten numbers from the MNIST dataset.

------

### 10. **Capsule Networks (CapsNets)**

- **What it does**: Addresses limitations of CNNs by understanding spatial relationships between features.
- **Example**: Better understanding the rotation or viewpoint of objects in images.

