### **Optimization Algorithms**

These algorithms help to find optimal solutions.

------

### **1. Gradient Descent**

- What it does

  :

  - Optimizes machine learning models by minimizing the error (or loss) function.
  - It adjusts model parameters (weights) in the direction of the steepest descent.

- Example

  :

  - **Linear Regression**: Finds the best-fit line by minimizing the difference between predicted and actual values.

------

### Types of Gradient Descent:

#### **a. Stochastic Gradient Descent (SGD)**

- **What it does**: Updates model parameters for each training example.

- Example

  :

  - **Real-Time Recommendation System**: Updates recommendations as new user data comes in.

#### **b. Mini-Batch Gradient Descent**

- **What it does**: Updates parameters using small batches of data instead of a single example or the entire dataset.

- Example

  :

  - **Image Classification**: Efficiently trains deep learning models like CNNs by processing batches of images.

#### **c. Adam (Adaptive Moment Estimation)**

- **What it does**: Combines the benefits of momentum and adaptive learning rates for faster convergence.

- Example

  :

  - **Natural Language Processing (NLP)**: Optimizes transformer-based models like BERT efficiently.

------

### **2. Genetic Algorithms**

- What it does

  :

  - Inspired by natural selection, it evolves solutions to optimization problems by combining and mutating the "fittest" solutions.

- Example

  :

  - **Route Optimization**: Used in logistics to find the shortest delivery route (e.g., for delivery services like Amazon).
  - **Game Development**: Designs AI opponents by evolving their strategies over time.

------

### **Key Differences**:

- **Gradient Descent**: Uses mathematical optimization to minimize errors iteratively.
- **Genetic Algorithms**: Mimics biological evolution to search for optimal solutions.